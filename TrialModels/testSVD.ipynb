{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2025041-b489-4ebb-a42b-df7c63fc5cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a283c904-d3eb-4ec6-9ed9-94f0667d1b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.06 s, sys: 126 ms, total: 2.19 s\n",
      "Wall time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "csv_files = [\n",
    "    # 'dataset/users_final_games1.csv',\n",
    "    'dataset/users_final_games2.csv',\n",
    "    'dataset/users_final_games3.csv'\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all CSVs\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "data.columns = [\n",
    "    \"ID\",\n",
    "    \"PlayerID\",\n",
    "    \"GameID\",\n",
    "    \"GameName\",\n",
    "    \"GameGenre\",\n",
    "    \"RunID\",\n",
    "    \"RunTime\",\n",
    "    \"CategoryType\",\n",
    "    \"PlayerCountry\",\n",
    "    \"PlayerPronouns\",\n",
    "    \"PlayerSignupDate\"\n",
    "]\n",
    "\n",
    "# Clean up\n",
    "data = data.drop_duplicates()\n",
    "data = data.dropna(subset=[\"PlayerID\", \"GameID\"])\n",
    "\n",
    "# Filter out cold-start users and games\n",
    "min_plays = 3\n",
    "\n",
    "active_users = data['PlayerID'].value_counts()[lambda x: x >= min_plays].index\n",
    "active_games = data['GameID'].value_counts()[lambda x: x >= min_plays].index\n",
    "\n",
    "data = data[data['PlayerID'].isin(active_users) & data['GameID'].isin(active_games)]\n",
    "\n",
    "# Step 2: Leave-One-Out Split\n",
    "# Keep only users with at least 2 interactions\n",
    "user_counts = data['PlayerID'].value_counts()\n",
    "valid_users = user_counts[user_counts >= 2].index\n",
    "data = data[data['PlayerID'].isin(valid_users)]\n",
    "\n",
    "# Split: keep one item out for test per user\n",
    "test_rows = (\n",
    "    data.groupby('PlayerID', group_keys=False)\n",
    "        .apply(lambda group: group.loc[group.sample(1, random_state=42).index])\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "train_data = pd.merge(data, test_rows, how='outer', indicator=True).query('_merge == \"left_only\"').drop(columns=['_merge'])\n",
    "\n",
    "# Step 3: Convert to Surprise format\n",
    "train_df = train_data[['PlayerID', 'GameID']].copy()\n",
    "train_df['Rating'] = 1\n",
    "\n",
    "test_df = test_rows[['PlayerID', 'GameID']].copy()\n",
    "test_df['Rating'] = 1  # still needed for evaluation format\n",
    "\n",
    "reader = Reader(rating_scale=(0, 1))\n",
    "trainset = Dataset.load_from_df(train_df, reader).build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96ab0010-468f-4714-a1f5-55945eee6633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Leave-One-Out Evaluation (SVD)\n",
      "Precision@10: 0.0031\n",
      "Recall@10:    0.0308\n",
      "Hit Rate@10:  0.0308\n",
      "NDCG@10:      0.0293\n",
      "CPU times: user 1min 46s, sys: 811 ms, total: 1min 47s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 4: Train the SVD model\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "# Step 5: Generate top-N recommendations per user (excluding seen items)\n",
    "all_game_ids = data['GameID'].unique()\n",
    "top_n = defaultdict(list)\n",
    "\n",
    "for uid in train_df['PlayerID'].unique():\n",
    "    user_games = set(train_df[train_df['PlayerID'] == uid]['GameID'])\n",
    "    candidate_games = [iid for iid in all_game_ids if iid not in user_games]\n",
    "\n",
    "    predictions = [model.predict(uid, iid) for iid in candidate_games]\n",
    "    top_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)[:10]\n",
    "\n",
    "    top_n[uid] = [(pred.iid, pred.est) for pred in top_predictions]\n",
    "\n",
    "# Step 6: Evaluate (Precision@10, Recall@10, Hit Rate@10, NDCG@10)\n",
    "def evaluate_leave_one_out(top_n, test_df):\n",
    "    actual = defaultdict(set)\n",
    "    for _, row in test_df.iterrows():\n",
    "        actual[row['PlayerID']].add(row['GameID'])\n",
    "\n",
    "    precisions, recalls, hits, ndcgs = [], [], [], []\n",
    "\n",
    "    for uid, recs in top_n.items():\n",
    "        recommended = [iid for iid, _ in recs]\n",
    "        relevant = actual.get(uid, set())\n",
    "\n",
    "        if not relevant:\n",
    "            continue\n",
    "\n",
    "        tp = set(recommended) & relevant\n",
    "        precision = len(tp) / len(recommended) if recommended else 0\n",
    "        recall = len(tp) / len(relevant) if relevant else 0\n",
    "        hit = 1 if tp else 0\n",
    "\n",
    "        # NDCG\n",
    "        dcg = sum(1 / np.log2(i + 2) for i, iid in enumerate(recommended) if iid in relevant)\n",
    "        ideal_dcg = sum(1 / np.log2(i + 2) for i in range(min(len(relevant), len(recommended))))\n",
    "        ndcg = dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        hits.append(hit)\n",
    "        ndcgs.append(min(ndcg, 1.0))  # clamp to 1\n",
    "\n",
    "    return np.mean(precisions), np.mean(recalls), np.mean(hits), np.mean(ndcgs)\n",
    "\n",
    "# Step 7: Report results\n",
    "precision, recall, hit_rate, ndcg = evaluate_leave_one_out(top_n, test_df)\n",
    "\n",
    "print(\"\\nðŸ“Š Leave-One-Out Evaluation (SVD)\")\n",
    "print(f\"Precision@10: {precision:.4f}\")\n",
    "print(f\"Recall@10:    {recall:.4f}\")\n",
    "print(f\"Hit Rate@10:  {hit_rate:.4f}\")\n",
    "print(f\"NDCG@10:      {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12b953f8-cf26-4a9b-a75a-60c4c4e83fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Training with: factors=50, lr=0.002, reg=0.02, epochs=20\n",
      "Precision@10: 0.0048 | Hit Rate@10: 0.0482 | NDCG@10: 0.0458\n",
      "\n",
      "ðŸ”§ Training with: factors=50, lr=0.002, reg=0.1, epochs=20\n",
      "Precision@10: 0.0047 | Hit Rate@10: 0.0472 | NDCG@10: 0.0446\n",
      "\n",
      "ðŸ”§ Training with: factors=50, lr=0.005, reg=0.02, epochs=20\n",
      "Precision@10: 0.0044 | Hit Rate@10: 0.0439 | NDCG@10: 0.0416\n",
      "\n",
      "ðŸ”§ Training with: factors=50, lr=0.005, reg=0.1, epochs=20\n",
      "Precision@10: 0.0029 | Hit Rate@10: 0.0294 | NDCG@10: 0.0274\n",
      "\n",
      "ðŸ”§ Training with: factors=100, lr=0.002, reg=0.02, epochs=20\n",
      "Precision@10: 0.0038 | Hit Rate@10: 0.0385 | NDCG@10: 0.0359\n",
      "\n",
      "ðŸ”§ Training with: factors=100, lr=0.002, reg=0.1, epochs=20\n",
      "Precision@10: 0.0038 | Hit Rate@10: 0.0380 | NDCG@10: 0.0359\n",
      "\n",
      "ðŸ”§ Training with: factors=100, lr=0.005, reg=0.02, epochs=20\n",
      "Precision@10: 0.0034 | Hit Rate@10: 0.0336 | NDCG@10: 0.0319\n",
      "\n",
      "ðŸ”§ Training with: factors=100, lr=0.005, reg=0.1, epochs=20\n",
      "Precision@10: 0.0038 | Hit Rate@10: 0.0377 | NDCG@10: 0.0353\n",
      "\n",
      "ðŸ† Best Precision@10: 0.0048 with params (50, 0.002, 0.02, 20)\n",
      "ðŸ† Best Hit Rate@10:  0.0482 with params (50, 0.002, 0.02, 20)\n",
      "ðŸ† Best Combined Score: 0.0265 with params (50, 0.002, 0.02, 20)\n",
      "CPU times: user 33min 52s, sys: 10.2 s, total: 34min 2s\n",
      "Wall time: 34min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100],\n",
    "    'lr_all': [0.002, 0.005],\n",
    "    'reg_all': [0.02, 0.1],\n",
    "    'n_epochs': [20]\n",
    "}\n",
    "\n",
    "# Track best scores\n",
    "best_precision = 0\n",
    "best_hit_rate = 0\n",
    "best_combined = 0\n",
    "best_params_precision = None\n",
    "best_params_hit = None\n",
    "best_params_combined = None\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_factors, lr, reg, n_epochs in itertools.product(\n",
    "    param_grid['n_factors'],\n",
    "    param_grid['lr_all'],\n",
    "    param_grid['reg_all'],\n",
    "    param_grid['n_epochs']\n",
    "):\n",
    "    print(f\"\\nðŸ”§ Training with: factors={n_factors}, lr={lr}, reg={reg}, epochs={n_epochs}\")\n",
    "    \n",
    "    model = SVD(n_factors=n_factors, lr_all=lr, reg_all=reg, n_epochs=n_epochs)\n",
    "    model.fit(trainset)\n",
    "\n",
    "    # Generate top-N recommendations\n",
    "    top_n = defaultdict(list)\n",
    "    for uid in train_df['PlayerID'].unique():\n",
    "        user_games = set(train_df[train_df['PlayerID'] == uid]['GameID'])\n",
    "        candidate_games = [iid for iid in all_game_ids if iid not in user_games]\n",
    "        predictions = [model.predict(uid, iid) for iid in candidate_games]\n",
    "        top_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)[:10]\n",
    "        top_n[uid] = [(pred.iid, pred.est) for pred in top_predictions]\n",
    "\n",
    "    # Evaluate\n",
    "    precision, recall, hit_rate, ndcg = evaluate_leave_one_out(top_n, test_df)\n",
    "\n",
    "    print(f\"Precision@10: {precision:.4f} | Hit Rate@10: {hit_rate:.4f} | NDCG@10: {ndcg:.4f}\")\n",
    "\n",
    "    # Store all results\n",
    "    results.append({\n",
    "        'n_factors': n_factors,\n",
    "        'lr_all': lr,\n",
    "        'reg_all': reg,\n",
    "        'n_epochs': n_epochs,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'hit_rate': hit_rate,\n",
    "        'ndcg': ndcg\n",
    "    })\n",
    "\n",
    "    # Track best for precision\n",
    "    if precision > best_precision:\n",
    "        best_precision = precision\n",
    "        best_params_precision = (n_factors, lr, reg, n_epochs)\n",
    "\n",
    "    # Track best for hit rate\n",
    "    if hit_rate > best_hit_rate:\n",
    "        best_hit_rate = hit_rate\n",
    "        best_params_hit = (n_factors, lr, reg, n_epochs)\n",
    "\n",
    "    # Optional: combined score (average)\n",
    "    combined_score = (precision + hit_rate) / 2\n",
    "    if combined_score > best_combined:\n",
    "        best_combined = combined_score\n",
    "        best_params_combined = (n_factors, lr, reg, n_epochs)\n",
    "\n",
    "# Final best scores\n",
    "print(f\"\\nðŸ† Best Precision@10: {best_precision:.4f} with params {best_params_precision}\")\n",
    "print(f\"ðŸ† Best Hit Rate@10:  {best_hit_rate:.4f} with params {best_params_hit}\")\n",
    "print(f\"ðŸ† Best Combined Score: {best_combined:.4f} with params {best_params_combined}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "199d4ec8-22af-4ff2-86b1-e7ca49f23a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Leave-One-Out Evaluation (SVD)\n",
      "Precision@10: 0.0027\n",
      "Recall@10:    0.0268\n",
      "Hit Rate@10:  0.0268\n",
      "NDCG@10:      0.0254\n"
     ]
    }
   ],
   "source": [
    "model2 = SVD(n_factors=50, lr_all=0.002, reg_all=0.02, n_epochs=20)\n",
    "model2.fit(trainset)\n",
    "\n",
    "# Step 5: Generate top-N recommendations per user (excluding seen items)\n",
    "all_game_ids = data['GameID'].unique()\n",
    "top_n = defaultdict(list)\n",
    "\n",
    "for uid in train_df['PlayerID'].unique():\n",
    "    user_games = set(train_df[train_df['PlayerID'] == uid]['GameID'])\n",
    "    candidate_games = [iid for iid in all_game_ids if iid not in user_games]\n",
    "\n",
    "    predictions = [model2.predict(uid, iid) for iid in candidate_games]\n",
    "    top_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)[:10]\n",
    "\n",
    "    top_n[uid] = [(pred.iid, pred.est) for pred in top_predictions]\n",
    "\n",
    "# Step 6: Evaluate (Precision@10, Recall@10, Hit Rate@10, NDCG@10)\n",
    "def evaluate_leave_one_out(top_n, test_df):\n",
    "    actual = defaultdict(set)\n",
    "    for _, row in test_df.iterrows():\n",
    "        actual[row['PlayerID']].add(row['GameID'])\n",
    "\n",
    "    precisions, recalls, hits, ndcgs = [], [], [], []\n",
    "\n",
    "    for uid, recs in top_n.items():\n",
    "        recommended = [iid for iid, _ in recs]\n",
    "        relevant = actual.get(uid, set())\n",
    "\n",
    "        if not relevant:\n",
    "            continue\n",
    "\n",
    "        tp = set(recommended) & relevant\n",
    "        precision = len(tp) / len(recommended) if recommended else 0\n",
    "        recall = len(tp) / len(relevant) if relevant else 0\n",
    "        hit = 1 if tp else 0\n",
    "\n",
    "        # NDCG\n",
    "        dcg = sum(1 / np.log2(i + 2) for i, iid in enumerate(recommended) if iid in relevant)\n",
    "        ideal_dcg = sum(1 / np.log2(i + 2) for i in range(min(len(relevant), len(recommended))))\n",
    "        ndcg = dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        hits.append(hit)\n",
    "        ndcgs.append(min(ndcg, 1.0))  # clamp to 1\n",
    "\n",
    "    return np.mean(precisions), np.mean(recalls), np.mean(hits), np.mean(ndcgs)\n",
    "\n",
    "# Step 7: Report results\n",
    "precision, recall, hit_rate, ndcg = evaluate_leave_one_out(top_n, test_df)\n",
    "\n",
    "print(\"\\nðŸ“Š Leave-One-Out Evaluation (SVD)\")\n",
    "print(f\"Precision@10: {precision:.4f}\")\n",
    "print(f\"Recall@10:    {recall:.4f}\")\n",
    "print(f\"Hit Rate@10:  {hit_rate:.4f}\")\n",
    "print(f\"NDCG@10:      {ndcg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newics635",
   "language": "python",
   "name": "newics635"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
